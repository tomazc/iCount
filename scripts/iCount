#!/usr/bin/env python

import os
import argparse

import iCount

parser = argparse.ArgumentParser(
    description='iCount - computational analysis of iCLIP data.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
parser.add_argument('--version', action='version',
                    version='%%(prog)s %s' % iCount.__version__)
subparsers = parser.add_subparsers(title='Pipeline commands')


# ### examples
def examples(args):
    iCount.examples.run()

parser_examples = iCount.analysis.params_to_argparse(subparsers,
                                                     iCount.examples)
parser_examples.set_defaults(func=examples)


# ### genomes and annotation

# releases: list all releases available on ensembl's FTP
#   Releases older than version 59 are not reported.
def releases(args):
    rel_list = iCount.genomes.releases.get()
    print('There are {:d} releases available: {:s}'.\
          format(len(rel_list), ','.join(rel_list)))


parser_releases = iCount.analysis.params_to_argparse(subparsers,
                                                     iCount.genomes.releases)
parser_releases.set_defaults(func=releases)


# species: list all species available on ensembl FTP
#   input: release
#   output: list of species for given release
def species(args):
    spe_list = iCount.genomes.species.get(release=args.release)
    print('There are {:d} species available: {:s}'.\
          format(len(spe_list), ','.join(spe_list)))


parser_species = iCount.analysis.params_to_argparse(subparsers,
                                                    iCount.genomes.species)
parser_species.set_defaults(func=species)


# annotation: download genome annotation for given release and species
#   (prepares a set of derived annotations used for annotating,
#    summarizations and analyses)
#
#   input: release
#   input: species
#   options:
#       - outdir: where to store file(s)
#       - filename: store into file with given name
#           (default: species.release.gtf.gz)
def annotation(args):
    gtf_file = iCount.genomes.annotation.get(
        release=args.release,
        species=args.species,
        target_dir=args.target_dir,
        target_fname=args.target_fname)

    print('Downloading annotation file: {:s}'.format(gtf_file))

parser_annotation = iCount.analysis.params_to_argparse(
    subparsers, iCount.genomes.annotation)

parser_annotation.set_defaults(func=annotation)


# sequence: download genome sequence for given release and species
#   input: release
#   input: species
#   options:
#       - outdir: where to store file(s)
#       - filename: store into file with given name
#           (default: species.release.fa.gz)

# examples: download example FASTQ files with iCLIP data
#   options:
#       - outdir: folder where to store downloaded files
def sequence(args):
    fasta_file = iCount.genomes.sequence.get(
        release=args.release,
        species=args.species,
        target_dir=args.target_dir,
        target_fname=args.target_fname,
        chromosomes=args.chromosomes)

    print('Downloading FASTA file: {:s}'.format(fasta_file))

parser_annotation = iCount.analysis.params_to_argparse(
    subparsers, iCount.genomes.sequence)

parser_annotation.set_defaults(func=sequence)


# segment: segment genome based on annotation file and save it into annotation
#   files
#   input: gtf file
#   output: gtf file with genetic regions
def segment(args):
    print('Reading genome annotation from {:s}'.format(args.annotation))
    iCount.genomes.segment.get_genes(gtf_in=args.annotation,
                                     gtf_out=args.segmentation_genes,
#                                     f_type = args.feature,
                                     attribute=args.attribute)
    print('Segmentation stored into {:s}'.format(args.segmentation_genes))

parser_segment = iCount.analysis.params_to_argparse(subparsers,
                                                    iCount.genomes.segment)

parser_segment.set_defaults(func=segment)


# ### demultiplexing

# demultiplex
#   input: FASTQ file with multiplexed (barcoded) samples
#   output: multiple FASTQ files, one for each sample
#   options:
#       - barcodes: comma-separated list of sample barcodes (including
#                   information on randomer positions)
#       - prefix: prefix of demultiplexed files
#       - mismatches: number of allowed mismatches in barcodes
#       - adapter: 3' adapter sequence to trim (default: AGATCGGAAGAGCGGTTCAG)
#       - outdir: output folder
def demultiplex(args):
    if not os.path.isdir(args.outdir):
        print('Error, output folder does not exist: {:s}'.format(args.outdir))
        print('Make sure that output folder exists.')
        return 1
    print('Demultiplexing file {:s}'.format(args.sequences))
    print('looking for barcodes {:s}'.format(', '.join(args.barcodes)))
    if args.adapter:
        print('trimming adapter {:s}'.format(args.adapter))
    print('allowing for {:d} mismatches in barcodes'.format(args.mismatches))
    print('discarding trimmed sequences shorter than {:d}'.format(
        args.minimum_length))
    out_fns = iCount.demultiplex.run(args.sequences, args.barcodes,
                                     args.adapter, args.mismatches,
                                     minimum_length=args.minimum_length,
                                     prefix=args.prefix,
                                     outdir=args.outdir)
    print('saved to files:')
    for fn in out_fns:
        print('   {:s}'.format(fn))

parser_demux = iCount.analysis.params_to_argparse(subparsers,
                                                  iCount.demultiplex)
parser_demux.set_defaults(func=demultiplex)


# ### mapping

# mapindex: prepare index for given genome assembly and annotation
#   input: genome sequence, annotation gtf
#   output: genome index files stored in genome index folder
#   options:
#       - genome index folder
def mapindex(args):
    if not os.path.isdir(args.outdir):
        print('Error, output folder does not exist: {:s}'.format(args.outdir))
        print('Make sure that output folder exists.')
        return 1
    return iCount.mapping.mapindex.run(args.genome, args.outdir,
                                       annotation_fname=args.annotation,
                                       overhang=args.overhang,
                                       threads=args.threads)

parser_mapindex = iCount.analysis.params_to_argparse(subparsers,
                                                     iCount.mapping.mapindex)
parser_mapindex.set_defaults(func=mapindex)


# map: call STAR and generate bam file
#   input: genome index, annotation, FASTQ file
#   output: bam file
#   options:
#       - number of allowed mismatches
def mapreads(args):
    if not os.path.isdir(args.outdir):
        print('Error, output folder does not exist: {:s}'.format(args.outdir))
        print('Make sure that output folder exists.')
        return 1
    return iCount.mapping.map.run(args.sequences, args.genome, args.outdir,
                                  annotation_fname=args.annotation,
                                  multimax=args.multimax,
                                  mismatches=args.mismatches,
                                  threads=args.threads)

parser_mapreads = iCount.analysis.params_to_argparse(subparsers,
                                                     iCount.mapping.map)
parser_mapreads.set_defaults(func=mapreads)


# xlsites: interpret bam and generate BED with data on identified sites
#   options:
#       -group: start, middle, end
def xlsites(args):
    print('Reading BAM file {:s}'.format(args.bam))
    print('grouping reads by their {:s}'.format(args.groupby))
    print('allowing {:d} mismatches in randomers'.format(args.mismatches))
    print('ignore reads with more than {:d} hits'.format(args.multimax))
    print('report on *{:s}* counts'.format(args.quant))
    s = iCount.mapping.xlsites.run(args.bam, args.unique, args.multi,
                                   group_by=args.groupby,
                                   quant=args.quant,
                                   randomer_mismatches=args.mismatches,
                                   multimax=args.multimax)
    if s is None:
        print('Error running command.')
        return 1

    all_recs, notmapped_recs, mapped_recs, lowmapq_recs, used_recs, \
    invalidrandomer_recs, norandomer_recs, bc_cn = s
    print('all records in BAM file: {:d}'.format(all_recs))
    print('reads not mapped: {:d}'.format(notmapped_recs))
    print('mapped reads records (hits): {:d}'.format(mapped_recs))
    print('hits ignored because with low MAPQ {:d}'.format(lowmapq_recs))
    print('records used for qunatification: {:d}'.format(used_recs))
    print('records with invalid randomer info in header: {:d}'.format(invalidrandomer_recs))
    print('records with no randomer info: {:d}'.format(norandomer_recs))
    print('ten most frequent randomers:')
    bc_cn = sorted([(cn, bc) for bc, cn in bc_cn.items()], reverse=True)
    for cn, bc in bc_cn[:10]:
        print('   {:s}: {:d}'.format(bc, cn))


    print('saved to BED file (uniquely mapped reads) {:s}'.format(args.unique))
    print('saved to BED file (multi-mapped reads) {:s}'.format(args.multi))

parser_xlsites = iCount.analysis.params_to_argparse(subparsers,
                                                    iCount.mapping.xlsites)
parser_xlsites.set_defaults(func=xlsites)


# filter:
#    input: set of bam files
#    output: set of reduced bam files (erroneous reads are removed)


# ### analyses
# annotate
#   input: annotation_file
#   input: cross_links_file
#   input: out_file
def annotate(args):
    out_file = iCount.analysis.annotate.annotate_cross_links(
        annotation_file=args.annotation_file,
        cross_links_file=args.cross_links_file,
        out_file=args.out_file,
        subtype=args.subtype,
        excluded_types=args.excluded_types)

parser_annotation = iCount.analysis.params_to_argparse(
    subparsers, iCount.analysis.annotate)

parser_annotation.set_defaults(func=annotate)


# summary: make summary report form annotation and cross-link file
#   input: annotation_file
#   input: cross_links_file
#   input: out_file
#   input: chrom_length_file
#   input: types_length_file

def summary(args):
    out_file = iCount.analysis.summary.make_summary_report(
        annotation_file=args.annotation_file,
        cross_links_file=args.cross_links_file,
        out_file=args.out_file,
        chrom_length_file=args.chrom_length_file,
        types_length_file=args.types_length_file,
        ndigits=args.ndigits,
        subtype=args.subtype,
        excluded_types=args.excluded_types)

parser_annotation = iCount.analysis.params_to_argparse(
    subparsers, iCount.analysis.summary)

parser_annotation.set_defaults(func=summary)


# group (bedGraphs)
#   input: BED6 files to be merged (with scores in 5th column)
#   output: one joined BED6 file (with summed scores for same sites)
#   options:
#       - outfile: filename to which the output should be written. If
#                    not given, write to stdout

def group(args):
    print("Grouping files:")
    for filename in args.files:
        print('\t* {}'.format(filename))
    out_file = iCount.analysis.group.run(
        files=args.files,
        outfile=args.outfile)
    if args.outfile:
        print("Results written to {}.".format(args.outfile))

parser_annotation = iCount.analysis.params_to_argparse(
    subparsers, iCount.analysis.group)
parser_annotation.set_defaults(func=group)


# peak identification
def peaks(args):
    print('Performing peak analysis on {:s}'.format(args.sites))
    print('using annotation from {:s}'.format(args.annotation))
    print('parameter values:')
    print('   hw={:d}'.format(args.hw))
    print('   fdr={:f}'.format(args.fdr))
    print('   perms={:d}'.format(args.perms))
    iCount.analysis.peaks.run(args.annotation, args.sites, args.peaks,
                              fout_scores=args.scores,
                              hw=args.hw, fdr=args.fdr, perms=args.perms,
                              features=args.features)
    print('Bed file with significant peaks saved to {:s}'.format(args.peaks))
    print('Scores for each cross-linked position saved to {:s}'.format(args.scores))

parser_peaks = iCount.analysis.params_to_argparse(subparsers,
                                                 iCount.analysis.peaks)
parser_peaks.set_defaults(func=peaks)


# clusters
def clusters(args):
    print('Performing cluster analysis on {:s}'.format(args.sites))
    print('parameter values:')
    print('   dist={:d}'.format(args.dist))
    iCount.analysis.clusters.run(args.sites, args.clusters, dist=args.dist)
    print('Bed file with clusters saved to {:s}'.format(args.clusters))

parser_clusters = iCount.analysis.params_to_argparse(subparsers,
                                                    iCount.analysis.clusters)
parser_clusters.set_defaults(func=clusters)


# kmer enrichment
def kmers(args):
    print(args)

parser_kmers = iCount.analysis.params_to_argparse(subparsers,
                                                 iCount.analysis.kmers)
parser_kmers.set_defaults(func=kmers)


# RNA maps
def rnamap(args):
    print(args)

parser_rnamap = iCount.analysis.params_to_argparse(subparsers,
                                                   iCount.analysis.rnamaps)
parser_rnamap.set_defaults(func=rnamap)


# ### parse and run it
parsed_args = parser.parse_args()
if not vars(parsed_args):
    parser.print_help()
    parser.exit(1)
parsed_args.func(parsed_args)
